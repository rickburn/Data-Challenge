# Examples Directory

This directory contains example scripts demonstrating the comprehensive logging, progress tracking, and validation systems implemented for the Lending Club ML Pipeline.

## Available Examples

### 1. `demo_logging_and_progress.py`

Comprehensive demonstration of all logging and progress tracking capabilities:

- **Comprehensive Logging System**: Shows how to set up structured logging with multiple handlers
- **Progress Tracking**: Demonstrates basic, nested, and batch progress tracking
- **Performance Monitoring**: Shows memory and timing monitoring
- **Data Validation**: Complete data quality and compliance validation
- **Complete Pipeline Integration**: End-to-end example combining all systems

#### Running the Demo

```bash
# From the project root directory
python examples/demo_logging_and_progress.py
```

#### Expected Output

The demo will:
1. âœ… Set up comprehensive logging system
2. ğŸš€ Show various progress tracking methods
3. âš¡ Demonstrate performance monitoring
4. ğŸ” Run data validation with sample data
5. ğŸ¯ Execute complete integrated pipeline

#### Generated Files

After running, check these directories:
- `logs/` - All execution logs with complete traceability
- `logs/pipeline_execution.log` - Main execution log
- `logs/operations/` - Structured operation logs (JSON format)
- `logs/performance/` - Performance metrics
- `logs/data_lineage.jsonl` - Complete data transformation lineage

## Features Demonstrated

### Logging Capabilities
- âœ… Multiple log handlers (console, file, structured JSON)
- âœ… Automatic function execution logging with `@log_execution` decorator
- âœ… Data lineage tracking for all transformations
- âœ… Performance metrics capture
- âœ… Error handling with full context

### Progress Tracking
- âœ… Basic progress bars with ETA calculation
- âœ… Nested progress tracking for complex operations
- âœ… Batch processing with automatic progress
- âœ… Memory usage and timing monitoring
- âœ… Integration with logging system

### Data Validation
- âœ… Comprehensive data quality checks
- âœ… Temporal constraint validation (critical for ML pipelines)
- âœ… Feature compliance validation (listing-time rule enforcement)
- âœ… Model performance validation
- âœ… Structured validation reporting

### Integration Features
- âœ… Complete traceability of all operations
- âœ… Automatic checkpoint creation
- âœ… Performance bottleneck detection
- âœ… Configuration-driven validation thresholds
- âœ… Error recovery and detailed error reporting

## Cursor Rules Compliance

All examples follow the cursor rules standards:

- **Type Hints**: All functions have complete type annotations
- **Docstrings**: Comprehensive numpy-style documentation
- **Logging**: Every operation is logged with full context
- **Progress Tracking**: All long-running operations show progress
- **Error Handling**: Comprehensive exception handling
- **Reproducibility**: Fixed random seeds and deterministic behavior
- **Data Lineage**: Complete transformation tracking
- **Performance Monitoring**: Resource usage tracking

## Next Steps

After reviewing these examples:

1. ğŸ“– Study the logging output in the `logs/` directory
2. ğŸ”§ Adapt the patterns to your specific pipeline components
3. ğŸ§ª Run the validation examples to understand data quality requirements
4. ğŸš€ Integrate these systems into your ML pipeline development

## Requirements

To run the examples, ensure you have:

```bash
pip install pandas numpy tqdm psutil pyyaml
```

## Support

These examples demonstrate the complete implementation of cursor rules standards for:
- Comprehensive traceability
- Professional progress tracking  
- Robust data validation
- Performance monitoring
- Error handling and recovery

Use these patterns throughout your ML pipeline development for production-ready code quality.
